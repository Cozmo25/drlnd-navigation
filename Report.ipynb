{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "This report details the methods and algorithms I used to solve the Navigation project as part of the Deep Reinforcement Learning Nanodegree from Udactiy.\n",
    "\n",
    "First step is to import the required Python packages to run the environment and algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import random\n",
    "import torch\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we instantiate the Unity Environment using the custom build provided by Udactiy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# Initialize the training environment\n",
    "env = UnityEnvironment(file_name=\"/Users/olliegraham/sites/ai/reinforcement-learning/deep-reinforcement-learning/p1_navigation/Banana.app\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create variables for the 'Brain' which will control the actions of the agent within the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section gives us an overview of the environment and shows us what the state space vector looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States look like: [1.         0.         0.         0.         0.84408134 0.\n",
      " 0.         1.         0.         0.0748472  0.         1.\n",
      " 0.         0.         0.25755    1.         0.         0.\n",
      " 0.         0.74177343 0.         1.         0.         0.\n",
      " 0.25854847 0.         0.         1.         0.         0.09355672\n",
      " 0.         1.         0.         0.         0.31969345 0.\n",
      " 0.        ]\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla DQN\n",
    "\n",
    "First we implement a vanilla deep Q - learning (DQN) agent, allow it to train on the environment until it is solved and then assess its performance.\n",
    "\n",
    "The vanilla DQN agent uses the following neural network architecture for both the target and online Q - learning models:\n",
    "\n",
    "![DQN](images/DQN-network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input size to the network is 1 x 37 states, which is passed to the first hidden layer of 64 fully connected neurons. The first fully connected layer passes to a second fully connected layer of 64 neurons and finally to the output layer which has 4 outputs corresponding to each of the possible actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Q - Learning agent assumes the following hyper-parameters:\n",
    "\n",
    "```\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size  \n",
    "BATCH_SIZE = 64         # minibatch size  \n",
    "GAMMA = 0.99            # discount factor  \n",
    "TAU = 1e-3              # for soft update of target parameters  \n",
    "LR = 5e-4               # learning rate  \n",
    "UPDATE_EVERY = 4        # how often to update the network  \n",
    "```\n",
    "\n",
    "We now initalize the Q-Learning agent, passing in the number of states, actions and a random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and initialize DQN agent with environment variables\n",
    "from dqn_agent import Agent\n",
    "agent = Agent(state_size=state_size, action_size=action_size, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we get to the main Q-Learning algorithm and training steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.46\n",
      "Episode 200\tAverage Score: 4.61\n",
      "Episode 300\tAverage Score: 7.57\n",
      "Episode 400\tAverage Score: 10.72\n",
      "Episode 497\tAverage Score: 13.05\n",
      "Environment solved in 397 episodes!\tAverage Score: 13.05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the agent with Deep Q Learning\n",
    "def dqn(n_episodes=2000, eps_start=1.0, eps_end=0.01, eps_decay=0.995, brain_name=brain_name, env=env, solve=False):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "        brain_name (object): controller for environment\n",
    "        env (objet): Unity ML Agent Banana collector environment\n",
    "        solve (bool): Boolean to determine if learning should stop once environment is solved\n",
    "        \n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "    state = env_info.vector_observations[0] # get the current state\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0] # get the current state\n",
    "        score = 0\n",
    "        while True:\n",
    "            action = agent.act(state, eps)\n",
    "            env_info = env.step(action)[brain_name]\n",
    "            next_state = env_info.vector_observations[0]\n",
    "            reward = env_info.rewards[0]\n",
    "            done = env_info.local_done[0]\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if solve:\n",
    "            if np.mean(scores_window)>=13.0:\n",
    "                print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "                torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "                break\n",
    "        else:\n",
    "            if i_episode == n_episodes:\n",
    "                print('\\nMax episodes reached!\\tFinal Average Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "                torch.save(agent.qnetwork_local.state_dict(), 'all-episodes-checkpoint.pth')\n",
    "    return scores\n",
    "\n",
    "scores = dqn(solve=True)\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Agent\n",
    "\n",
    "There is a lot going on in the cell above, so I will summarise what is happening here.\n",
    "\n",
    "1. We define the Q-learning function and arguments for number of episodes, epsilon start, epsilon end, epsilon decay rate, the brain name, the environment, and a boolean solve.\n",
    "    * No. of episodes - tells the algorithm how many epsidoes we'd like to train for if the agent doesn't solve the environment before this number is reached\n",
    "    * Epsilon start - the starting probability for the agent to choose a random action\n",
    "    * Epsilon end - the lowest probability for the agent to choose a random action\n",
    "    * Epsilon decay - the rate at which epsilon should decay from episode to episode down to epsilon end\n",
    "    * The brain name - is the controller for the environment\n",
    "    * The environment - is the Banana collector environment that we wish to train the agent in\n",
    "    * Solve - whether the training should stop once the environment is solved, or continue until the max number of episodes is reached\n",
    "    \n",
    "2. Next we set variables for the score of each episode, a container for the score over the last 100 epsiodes, initial epsilon value, reset the environment and get the initial state info.\n",
    "3. Then, for each episode we:\n",
    "    * Reset the environment and the score\n",
    "    ```\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        score = 0\n",
    "    ```\n",
    "    * Get the current state\n",
    "    ```\n",
    "        state = env_info.vector_observations[0] # get the current state\n",
    "    ```\n",
    "    * Until the episode ends we:\n",
    "        * Get the agent to take an action which is chosen at random with probability epsilon, or by the greedy policy as learned by the online network if a sufficient number of episodes have passed\n",
    "        ```\n",
    "            action = agent.act(state, eps)\n",
    "        ```\n",
    "        * Pass the action to the environment and get the next state\n",
    "        ```\n",
    "            env_info = env.step(action)[brain_name]\n",
    "            next_state = env_info.vector_observations[0]\n",
    "        ```\n",
    "        * Receive a reward for taking the previous action and arriving in the next state\n",
    "        ```\n",
    "            reward = env_info.rewards[0]\n",
    "        ```\n",
    "        * Check if the episode is done\n",
    "        ```\n",
    "            done = env_info.local_done[0]\n",
    "        ```\n",
    "        * Step the agent forward 1 step, the agent will then store the state, action, reward, next state, done tuple in memory for use in learning later (up to the maximum memory size as set in the agent parameters)\n",
    "        ```\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "        ```\n",
    "        * Set the current state to be the next state\n",
    "        ```\n",
    "            state = next_state\n",
    "        ```\n",
    "        * Update the score recieved for the most recent reward\n",
    "        ```\n",
    "            score += reward\n",
    "        ```\n",
    "        * Check if the episode has ended\n",
    "        * Update the total scores for the episode and the rolling 100 episode window\n",
    "        ```\n",
    "            scores_window.append(score)       # save most recent score\n",
    "            scores.append(score)\n",
    "            \n",
    "        ```\n",
    "        * Finally we decay epsilon by the decay paramter - then we move to next episode if the environment has not been solved (and solved is set to True)\n",
    "        * If the environment is solved, or the total number of episodes are reached print the results of the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent will randomly sample a mini-batch of tuples from the S,A,R,S,A tuples stored in memory in order to learn from the actions it has taken in the various states it has observed, this happens every X steps where X is set to 4 in the agent parameters above.\n",
    "\n",
    "The agent then uses Q-values generated from the target network and expected Q-values from the online network as input to the Bellman equation as follows:\n",
    "\n",
    "<img src=\"images/fixed-q-target.png\" width=\"75%\">\n",
    "\n",
    "I utilsed the smooth_l1_loss function to be used as the error term for the neural network to minimise on the target network. Smooth L1 Loss is described as follows:\n",
    "\n",
    "Creates a criterion that uses a squared term if the absolute element-wise error falls below 1 and an L1 term otherwise. It is less sensitive to outliers than the MSELoss and in some cases prevents exploding gradients (e.g. see “Fast R-CNN” paper by Ross Girshick). Also known as the Huber loss:\n",
    "\n",
    "![huber loss](images/Huber-loss.png)\n",
    "\n",
    "where zi is given by:\n",
    "\n",
    "![zi](images/zi.png)\n",
    "\n",
    "x and y arbitrary shapes with a total of n elements each the sum operation still operates over all the elements, and divides by n.\n",
    "\n",
    "We then do a 'soft copy' of the network weights from the online network to the target network in accordance with the following formula:\n",
    "\n",
    "```\n",
    "    θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "```\n",
    "\n",
    "Where Tau was defined in the Agent parameters above.\n",
    "\n",
    "Over the course of many episodes the agent will use the memories of S,A,R,S,A sequences it has obsvered to form an optimal policy which becomes encoded in the weights of the online and target neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "I experimented with several settings for both hyper-parameters of the agent and the structure of the neural networks in order to see how quickly the agent was able to solve the environment.\n",
    "\n",
    "All of the following experiments were run with these arguments: **max number of episodes=2000, epsilon start=1.0, epsilon end=0.01, eps_decay=0.995.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vanilla DQN**\n",
    "\n",
    "Initial agent parameters:\n",
    "\n",
    "```\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size  \n",
    "BATCH_SIZE = 64         # minibatch size  \n",
    "GAMMA = 0.99            # discount factor  \n",
    "TAU = 1e-3              # for soft update of target parameters  \n",
    "LR = 5e-4               # learning rate  \n",
    "UPDATE_EVERY = 4        # how often to update the network \n",
    "```\n",
    "\n",
    "Neural Network:  \n",
    "\n",
    "Fully connected layer 1 number of neurons = 64  \n",
    "Fully connected layer 2 number of neurons = 64  \n",
    "\n",
    "Algorithm:  \n",
    "Vanilla Deep Q-Learning\n",
    "\n",
    "Training Results (Baseline):\n",
    "\n",
    "**Solved in 391 epsiodes**\n",
    "![DQN](images/DQN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Double DQN**\n",
    "\n",
    "Agent parameters:\n",
    "\n",
    "```\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size  \n",
    "BATCH_SIZE = 64         # minibatch size  \n",
    "GAMMA = 0.99            # discount factor  \n",
    "TAU = 1e-3              # for soft update of target parameters  \n",
    "LR = 5e-4               # learning rate  \n",
    "UPDATE_EVERY = 4        # how often to update the network \n",
    "```\n",
    "\n",
    "Neural Network:  \n",
    "\n",
    "Fully connected layer 1 number of neurons = 64  \n",
    "Fully connected layer 2 number of neurons = 64  \n",
    "\n",
    "Algorithm:  \n",
    "Double Deep Q-Learning (DDQN) - here the target network is used to generate the Q-values for the optimal actions provided by the online network. Those Q-values are then fed into the Bellman equation to update the Q-Targets as input for the loss minimisation step of the online network\n",
    "\n",
    "<img src=\"images/double-DQN-equation.png\" width=\"60%\">\n",
    "\n",
    "Training Results:\n",
    "\n",
    "**Solved in 384 epsiodes**\n",
    "<img src=\"images/DDQN.png\">\n",
    "![DDQN](images/DDQN-Chart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dueling - Double DQN**\n",
    "\n",
    "Agent parameters:\n",
    "\n",
    "```\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size  \n",
    "BATCH_SIZE = 64         # minibatch size  \n",
    "GAMMA = 0.99            # discount factor  \n",
    "TAU = 1e-3              # for soft update of target parameters  \n",
    "LR = 5e-4               # learning rate  \n",
    "UPDATE_EVERY = 4        # how often to update the network \n",
    "```\n",
    "\n",
    "Neural Network:  \n",
    "\n",
    "Fully connected layer 1 number of neurons = 64  \n",
    "Fully connected layer 2 number of neurons = 64  \n",
    "Fully connected (advantage) layer 3 number of neurons = 4 (number of actions)  \n",
    "Fully connected (value) layer 3 number of neurons = 1\n",
    "\n",
    "Algorithm:  \n",
    "Dueling Double Deep Q-Learning (DDQN) - Here we decompose the Q(s,a) value into V(s) of being in that state and A(s,a) the advantage of taking any action versus all the other actions in that state.\n",
    "\n",
    "<img src=\"images/qsa-vs-asa.png\" width=\"40%\">\n",
    "\n",
    "We can then split the final fully connected layer into two branches one for the advantage and one for the value as follows to get a Q value for each action and a single value for V(s):\n",
    "\n",
    "<img src=\"images/dueling-ddqn-architecture.png\" width=\"50%\">\n",
    "\n",
    "The results are then recombined according to the following equation:\n",
    "\n",
    "<img src=\"images/dueling-ddqn-combination.png\" width=\"60%\">\n",
    "\n",
    "The revised neural network architecture is as follows:\n",
    "\n",
    "<img src=\"images/Dueling-DQN-Network.png\">\n",
    "\n",
    "Training Results:\n",
    "\n",
    "**Solved in 401 epsiodes**\n",
    "![DDQN](images/Dueling-DDQN.png)\n",
    "![DDQN](images/Dueling-DDQN-Chart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment\n",
    "\n",
    "All of the results of experiments thus far have yielded a similar time in episodes for the agent to solve the environment, with the main change being how stable the learning has become as we added the Q-learning improvements to the model and network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dueling - Double DQN. Increased Batch Size**\n",
    "\n",
    "Agent parameters:\n",
    "\n",
    "```\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size  \n",
    "BATCH_SIZE = 128         # minibatch size  \n",
    "GAMMA = 0.99            # discount factor  \n",
    "TAU = 1e-3              # for soft update of target parameters  \n",
    "LR = 5e-4               # learning rate  \n",
    "UPDATE_EVERY = 4        # how often to update the network \n",
    "```\n",
    "\n",
    "Neural Network:  \n",
    "\n",
    "Fully connected layer 1 number of neurons = 64  \n",
    "Fully connected layer 2 number of neurons = 64  \n",
    "Fully connected (advantage) layer 3 number of neurons = 4 (number of actions)  \n",
    "Fully connected (value) layer 3 number of neurons = 1\n",
    "\n",
    "Algorithm:  \n",
    "Dueling Double Deep Q-Learning (DDQN)\n",
    "\n",
    "**Solved in 372 epsiodes**\n",
    "<img src=\"images/Dueling-DDQN-Batch-128v2.png\">\n",
    "![DDQN](images/Dueling-DDQN-Chart-Batch-128.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dueling - Double DQN. Increased Batch Size + Increased Tau**\n",
    "\n",
    "Agent parameters:\n",
    "\n",
    "```\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size  \n",
    "BATCH_SIZE = 128         # minibatch size  \n",
    "GAMMA = 0.99            # discount factor  \n",
    "TAU = 1e-2              # for soft update of target parameters  \n",
    "LR = 5e-4               # learning rate  \n",
    "UPDATE_EVERY = 4        # how often to update the network \n",
    "```\n",
    "\n",
    "Neural Network:  \n",
    "\n",
    "Fully connected layer 1 number of neurons = 64  \n",
    "Fully connected layer 2 number of neurons = 64  \n",
    "Fully connected (advantage) layer 3 number of neurons = 4 (number of actions)  \n",
    "Fully connected (value) layer 3 number of neurons = 1\n",
    "\n",
    "Algorithm:  \n",
    "Dueling Double Deep Q-Learning (DDQN)\n",
    "\n",
    "**Solved in 414 epsiodes**\n",
    "![DDQN](images/dueling-ddqn-increased-tau2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment\n",
    "\n",
    "Increasing Tau - or the rate at which we incorporate learning from the online to the target network seems to decrease the speed at which the agent solves the environment vs. increased batch size alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then tried experimenting with the number of neurons in the layers of the neural network with a suspicion that with less parameters to train the network might be able to learn faster. Here is a diagram of the revised neural network:\n",
    "\n",
    "<img src=\"images/dueling-ddqn-32-hidden-architecture.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dueling - Double DQN. Increased Batch Size + Increased Tau + 32 hidden units**\n",
    "\n",
    "Agent parameters:\n",
    "\n",
    "```\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size  \n",
    "BATCH_SIZE = 128         # minibatch size  \n",
    "GAMMA = 0.99            # discount factor  \n",
    "TAU = 1e-2              # for soft update of target parameters  \n",
    "LR = 5e-4               # learning rate  \n",
    "UPDATE_EVERY = 4        # how often to update the network \n",
    "```\n",
    "\n",
    "Neural Network:  \n",
    "\n",
    "Fully connected layer 1 number of neurons = 32  \n",
    "Fully connected layer 2 number of neurons = 32  \n",
    "Fully connected (advantage) layer 3 number of neurons = 4 (number of actions)  \n",
    "Fully connected (value) layer 3 number of neurons = 1\n",
    "\n",
    "Algorithm:  \n",
    "Dueling Double Deep Q-Learning (DDQN)\n",
    "\n",
    "**Solved in 410 epsiodes**\n",
    "<img src=\"images/dueling-ddqn-increased-tau.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment\n",
    "\n",
    "Changing the number of hidden units seemed to have little impact on the speed of solving the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally I looked at increasing the rate of epsilon decay, under the hypothesis that given the limited number of states in the environment the agent might not need to take exploratory actions for many steps in order to build a near optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dueling - Double DQN. Increased Batch Size + Increased Tau + 32 hidden units + Epsilon Decay = 0.95**\n",
    "\n",
    "Agent parameters:\n",
    "\n",
    "```\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size  \n",
    "BATCH_SIZE = 128         # minibatch size  \n",
    "GAMMA = 0.99            # discount factor  \n",
    "TAU = 1e-2              # for soft update of target parameters  \n",
    "LR = 5e-4               # learning rate  \n",
    "UPDATE_EVERY = 4        # how often to update the network \n",
    "```\n",
    "\n",
    "Neural Network:  \n",
    "\n",
    "Fully connected layer 1 number of neurons = 32  \n",
    "Fully connected layer 2 number of neurons = 32  \n",
    "Fully connected (advantage) layer 3 number of neurons = 4 (number of actions)  \n",
    "Fully connected (value) layer 3 number of neurons = 1\n",
    "\n",
    "Algorithm:  \n",
    "Dueling Double Deep Q-Learning (DDQN)\n",
    "\n",
    "**Solved in 286 epsiodes**\n",
    "<img src=\"images/dueling-ddqn-eps_decay0.95.png\" width=\"55%\">\n",
    "<img src=\"images/dueling-ddqn-eps_decay0.95-chart.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment\n",
    "\n",
    "This is the biggest improvement in number of episodes to solve the environment so far! I then continued with several experiments, each increasing the amount of epsilon decay, to a final epsilon decay of 0.7 and my optimal agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dueling - Double DQN. Increased Batch Size + Increased Tau + 32 hidden units + Epsilon Decay = 0.7**\n",
    "\n",
    "Agent parameters:\n",
    "\n",
    "```\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size  \n",
    "BATCH_SIZE = 128         # minibatch size  \n",
    "GAMMA = 0.99            # discount factor  \n",
    "TAU = 1e-2              # for soft update of target parameters  \n",
    "LR = 5e-4               # learning rate  \n",
    "UPDATE_EVERY = 4        # how often to update the network \n",
    "```\n",
    "\n",
    "Neural Network:  \n",
    "\n",
    "Fully connected layer 1 number of neurons = 32  \n",
    "Fully connected layer 2 number of neurons = 32  \n",
    "Fully connected (advantage) layer 3 number of neurons = 4 (number of actions)  \n",
    "Fully connected (value) layer 3 number of neurons = 1\n",
    "\n",
    "Algorithm:  \n",
    "Dueling Double Deep Q-Learning (DDQN)\n",
    "\n",
    "**Solved in 126 epsiodes**\n",
    "<img src=\"images/dueling-ddqn-eps_decay0.7-plus-chart.png\" width=\"55%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "It seems as though this agent is able to score 13 points required to solve the environment in approximately 50 episodes, so any exploration beyond this point only acts to increase the time taken to solve the environment. This is likely due to the environment's relative simplicity and the limited number of actions the agent can take in the environment.\n",
    "\n",
    "As a result, decaying epsilon quickly to the minimum value allows the agent to act greedily as soon as possible and reach the 13 point average score over 100 episodes in the shortest period.\n",
    "\n",
    "It may be possible to further improve the performance of the agent by implementing Q-learning improvements such as: prioritised experience replay (boosting the sampling of experiences which are likely to lead to the greatest improvements in the error term of the network and thus the estimation of Q-values), or Dyna-Q which blends Q-planning and Q-learning so that the agent is able to build a model of the environment as it learns, thus drastically reducing the number ineractions the agent needs to have with its environment before it learns a near optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watch the trained agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click on the screenshot below to watch my best performing agent act in the Unity Banana Collector environment.\n",
    "\n",
    "<a href=\"http://www.youtube.com/watch?feature=player_embedded&v=GFSBQ08WmTQ\n",
    "\" target=\"_blank\"><img src=\"http://img.youtube.com/vi/GFSBQ08WmTQ/0.jpg\" \n",
    "alt=\"Youtube Video of Agent in Environment\" width=\"400\" height=\"300\" border=\"10\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have set up the environment per the instructions in the README file you can watch the agent interact with the environment by running the code below. \n",
    "\n",
    "Don't forget to close the environment when you're finished!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an agent and watch it act\n",
    "\n",
    "# load the weights from file\n",
    "agent.qnetwork_local.load_state_dict(torch.load('dueling-ddqn-checkpoint-solved-126-nn-change-0.7eps_decay.pth'))\n",
    "\n",
    "state = env_info.vector_observations[0] # get the current state\n",
    "\n",
    "for i in range(3):\n",
    "    env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "    score = 0\n",
    "    for j in range(200):\n",
    "        action = agent.act(state)\n",
    "        env_info = env.step(action)[brain_name]\n",
    "        state = env_info.vector_observations[0]\n",
    "        reward = env_info.rewards[0]\n",
    "        done = env_info.local_done[0]\n",
    "        score += reward\n",
    "        if done:\n",
    "            break \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the environment when done\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
